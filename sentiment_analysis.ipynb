{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 11147\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "\n",
    "import json\n",
    "\n",
    "X_text = []\n",
    "y_text = []\n",
    "\n",
    "with open(\"reviews.jl\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    parsed_json = json.loads(line)\n",
    "\n",
    "    X_text.append(parsed_json[\"text\"])\n",
    "    y_text.append(parsed_json[\"rating\"])\n",
    "\n",
    "print(\"Size of the dataset: {}\".format(len(X_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RATING: 5\nTEXT: e' un ottimo traduttore. Eccezzionale il fatto di poterci scrivere in cinese. se si e' spesso in cina e' indispensabile.\n\nRATING: 4\nTEXT: Non ho dato il massimo di valutazione perchè non ho ancora fatto il backup del vecchio hard disk e quindi non l'ho messo alla prova. Devo soltanto dire che la consegna è stata perfetta, come sempre, rispetta al massimo la descrizione del prodotto e poi devo aggiungere che difficilmente potrò dare un giudizio negativo,perchè il samsung SSD 850 EVO è uno dei migliori. A lavoro finito ,nel caso dovessi trovare dei difetti, aggiungerò immediatamente le note negative.\n\nRATING: 3\nTEXT: Non ha la stessa resa che ha il \"leggi tutto\" che tutti conosciamo....VLC. Spero che ci siano aggiornamenti che lo rendano piu fruibile. Per ora poco utilizzo su Fire 2015. Confido le cose cambino presto!!\n\n"
     ]
    }
   ],
   "source": [
    "#View random records\n",
    "\n",
    "print(\"RATING: {}\\nTEXT: {}\\n\".format(y_text[3],X_text[3]))\n",
    "print(\"RATING: {}\\nTEXT: {}\\n\".format(y_text[20],X_text[20]))\n",
    "print(\"RATING: {}\\nTEXT: {}\\n\".format(y_text[450],X_text[450]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 1, 2, 3, 4, 5\n\nLabel\tType Label\t\tTransformed\tType Transformed\n1\t<class 'str'>\t\t0\t\t<class 'numpy.int64'>\n2\t<class 'str'>\t\t1\t\t<class 'numpy.int64'>\n3\t<class 'str'>\t\t2\t\t<class 'numpy.int64'>\n4\t<class 'str'>\t\t3\t\t<class 'numpy.int64'>\n5\t<class 'str'>\t\t4\t\t<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "#Encode the labels\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(y_text)\n",
    "    \n",
    "print(\"Classes: \" + \", \".join(le.classes_) + \"\\n\")\n",
    "\n",
    "print(\"Label\\tType Label\\t\\tTransformed\\tType Transformed\")\n",
    "\n",
    "for c in le.classes_:\n",
    "    print(\"{}\\t{}\\t\\t{}\\t\\t{}\".format(c,\n",
    "                                  type(str(c)),\n",
    "                                  le.transform([str(c)])[0],\n",
    "                                  type(le.transform([str(c)])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train size: \t10032\ny train size: \t10032\nX test size: \t1115\ny test size: \t1115\n"
     ]
    }
   ],
   "source": [
    "#Create training and testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(X_text,\n",
    "                                                                        y_text,\n",
    "                                                                        test_size=test_size,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "print(\"X train size: \\t{}\".format(len(X_train_text)))\n",
    "print(\"y train size: \\t{}\".format(len(y_train_text)))\n",
    "print(\"X test size: \\t{}\".format(len(X_test_text)))\n",
    "print(\"y test size: \\t{}\".format(len(y_test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the classifier\n",
    "\n",
    "text_clf = text_clf.fit(X_train_text, le.transform(y_train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t1, 4, 4, 5, 4, 1, 1, 4, 5, 5, 5, 5, 5, 1, 1\nTrue values:\t1, 3, 4, 5, 4, 3, 3, 5, 5, 3, 4, 5, 5, 1, 1\n"
     ]
    }
   ],
   "source": [
    "#Make prediction on the test set\n",
    "y_pred = text_clf.predict(X_test_text)\n",
    "\n",
    "y_pred_text = le.inverse_transform(y_pred)\n",
    "\n",
    "#Print comparison of first 15 elements predicted and true\n",
    "print(\"Prediction:\\t\" + \", \".join(y_pred_text[0:15]))\n",
    "print(\"True values:\\t\" + \", \".join(y_test_text[0:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute F1-score for each class\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = le.transform(y_test_text)\n",
    "\n",
    "f1_s = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "\n",
    "#Create Random Agent Classifier prediction and F1-score\n",
    "\n",
    "import random\n",
    "\n",
    "y_pred_random_agent = []\n",
    "for _ in range(len(y_pred)):\n",
    "    y_pred_random_agent += random.choice(le.classes_) \n",
    "\n",
    "f1_s_random_agent = f1_score(y_true, le.transform(y_pred_random_agent), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a similar classifier using an equivalent english dataset\n",
    "\n",
    "import json\n",
    "\n",
    "X_text_eng = []\n",
    "y_text_eng = []\n",
    "\n",
    "with open(\"reviews_eng.jl\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "#Same length of the Italian dataset\n",
    "for line in lines:\n",
    "\n",
    "    parsed_json = json.loads(line)\n",
    "\n",
    "    X_text_eng.append(parsed_json[\"reviewText\"])\n",
    "    y_text_eng.append(str(parsed_json[\"overall\"])[0])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_text_eng, X_test_text_eng, y_train_text_eng, y_test_text_eng =\\\n",
    "    train_test_split(X_text_eng, y_text_eng, test_size=test_size, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_eng = LabelEncoder()\n",
    "\n",
    "le_eng.fit(y_train_text_eng)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_eng = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier()), ])\n",
    "\n",
    "text_clf_eng = text_clf_eng.fit(X_train_text_eng, le_eng.transform(y_train_text_eng))\n",
    "\n",
    "#Compiute F1-score of the English classifier\n",
    "y_pred_eng = text_clf_eng.predict(X_test_text_eng)\n",
    "\n",
    "y_true_eng = le_eng.transform(y_test_text_eng)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_s_eng = f1_score(y_true_eng, y_pred_eng, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create another classifier using an english dataset 10 times bigger than the previous\n",
    "\n",
    "import json\n",
    "\n",
    "X_text_eng_big = []\n",
    "y_text_eng_big = []\n",
    "\n",
    "with open(\"reviews_eng_big.jl\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    parsed_json = json.loads(line)\n",
    "\n",
    "    X_text_eng_big.append(parsed_json[\"reviewText\"])\n",
    "    y_text_eng_big.append(str(parsed_json[\"overall\"])[0])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_text_eng_big, X_test_text_eng_big, y_train_text_eng_big, y_test_text_eng_big =\\\n",
    "    train_test_split(X_text_eng_big, y_text_eng_big, test_size=0.1, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_eng_big = LabelEncoder()\n",
    "\n",
    "le_eng_big.fit(y_train_text_eng_big)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_eng_big = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier()), ])\n",
    "\n",
    "text_clf_eng_big = text_clf_eng_big.fit(X_train_text_eng_big, le_eng_big.transform(y_train_text_eng_big))\n",
    "\n",
    "#Compiute F1-score of the Big English classifier\n",
    "y_pred_eng_big = text_clf_eng_big.predict(X_test_text_eng_big)\n",
    "\n",
    "y_true_eng_big = le_eng_big.transform(y_test_text_eng_big)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_s_eng_big = f1_score(y_true_eng_big, y_pred_eng_big, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the classifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'vect__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "         'vect__strip_accents': ('ascii', 'unicode', None),\n",
    "         'tfidf__norm': ('l1', 'l2', None),\n",
    "         'clf__alpha': (0.0001, 0.0003, 0.001),\n",
    "         'clf__loss': ('hinge', 'squared_hinge', 'perceptron')}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, param, n_jobs=-1)\n",
    "\n",
    "text_clf_gs = gs_clf.fit(X_train_text, le.transform(y_train_text))\n",
    "\n",
    "y_pred_gs = text_clf_gs.predict(X_test_text)\n",
    "\n",
    "#Compute F1-score, Precision ans Recall\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_true = le.transform(y_test_text)\n",
    "\n",
    "precision_gs = precision_score(y_true, y_pred_gs, average=None)\n",
    "\n",
    "recall_gs = recall_score(y_true, y_pred_gs, average=None)\n",
    "\n",
    "f1_s_gs = f1_score(y_true, y_pred_gs, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBest parameters:\n{'clf__alpha': 0.0003, 'clf__loss': 'hinge', 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 3), 'vect__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest parameters:\")\n",
    "print(text_clf_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\tF1 Italian\tF1 Tuned\tF1 Random\tF1 English\tF1 English Big\n1-Star\t0.5394\t0.5723\t0.1680\t0.4706\t0.5159\n2-Star\t0.3271\t0.2917\t0.1505\t0.0333\t0.0734\n3-Star\t0.3333\t0.4408\t0.1772\t0.1698\t0.0980\n4-Star\t0.3297\t0.3846\t0.2231\t0.2448\t0.1612\n5-Star\t0.6488\t0.6625\t0.2463\t0.8002\t0.7913\n"
     ]
    }
   ],
   "source": [
    "#Compare results\n",
    "\n",
    "print(\"Label\\tF1 Italian\\tF1 Tuned\\tF1 Random\\tF1 English\\tF1 English Big\")\n",
    "for i in range(len(le.classes_)):\n",
    "    print(\"{}-Star\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(le.classes_[i],\n",
    "                                                                   f1_s[i],\n",
    "                                                                   f1_s_gs[i],\n",
    "                                                                   f1_s_random_agent[i],\n",
    "                                                                   f1_s_eng[i],\n",
    "                                                                   f1_s_eng_big[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1-Star\t2-Star\t3-Star\t4-Star\t5-Star\n1-Star\t89\t20\t8\t11\t24\n2-Star\t44\t35\t21\t14\t21\n3-Star\t31\t10\t50\t37\t55\n4-Star\t8\t8\t25\t75\t156\n5-Star\t6\t6\t13\t46\t302\n"
     ]
    }
   ],
   "source": [
    "#create confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\t\" + \"-Star\\t\".join(le.classes_) + \"-Star\")\n",
    "for i in range(len(le.classes_)):\n",
    "    print(le.classes_[i] + \"-Star\\t\" + \"\\t\".join(str(x) for x in cm[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\tPrecision\tRecall\n1-Star\t0.50\t0.59\n2-Star\t0.44\t0.26\n3-Star\t0.43\t0.27\n4-Star\t0.41\t0.28\n5-Star\t0.54\t0.81\n"
     ]
    }
   ],
   "source": [
    "#Compute precision and recall for the classes\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "\n",
    "print(\"Label\\tPrecision\\tRecall\")\n",
    "for i in range(len(le.classes_)):\n",
    "    print(\"{}-Star\\t{:.2f}\\t{:.2f}\".format(le.classes_[i],\n",
    "                                              precision[i],\n",
    "                                              recall[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}